# Config file for jet classification

#------------------------------------------------------------------
# These following parameters are used only in ML analysis
#------------------------------------------------------------------

# Which dataset/Jet Classification problem to use
classification_task: 'ZvsQCD' # Choices: 'qvsg', 'ZvsQCD'. The 'qvsg' dataset: EnergyFlow. The 'ZvsQCD' dataset: JetClass

# Size of labeled data to load (i.e. number of jets). Recommendet split: 80/10/10
# The qvsg dataset has 2M jets, the ZvsQCD dataset currently has 2M jets, but we can download more if needed (although training would be tough). 
n_train: 1600
n_val:   200
n_test:  200

# Select model: subjet_gcn_pytorch, subjet_gat_pytorch, particle_gcn_pytorch, particle_gat_pytorch, 
#               particle_transformer, particle_net_laman, particle_transformer, particle_transformer_graph
models: ['particle_transformer_graph']

particle_transformer:
    graph_types: ['disconnected'] # placeholder name for the graph type since the fully connected graph is constructed in the architecture script 
    batch_size: 256
    epochs: 18
    learning_rate: 0.001
    input_dim:        4 # number of input features, if 4: use 4dim feature space (pt, eta, phi, mass). If 1: use only the  pt of each particle.
    pair_input_dim: 4 # how many interaction terms for pair of particles. If 3: use (dR, k_t = min(pt_1, pt_2)*dR, z = min(pt_1, pt_2)/(pt_1 + pt_2) ), if 4: also use m^2 = (E1 + E2)^2 - (p1 + p2)^2    
    load_model: False # if True, load the pre-trained model from the path specified in the architecture script. Else, train the model from scratch.
    save_model: False # if True, save the trained model to the path specified in the architecture script. Else, do not save the model.

particle_transformer_graph:
    graph_types: ['disconnected'] # placeholder name for the graph type since the fully connected graph is constructed in the architecture script 
    batch_size: 128
    epochs: 15
    learning_rate: 0.001
    input_dim: 1                         # number of input features, if 4: use 4dim feature space (pt, eta, phi, mass). If 1: use only the  pt of each particle.
    pair_input_dim: 4 
    load_model: False                    # if True, load the pre-trained model from the path specified in the architecture script. Else, train the model from scratch.
    save_model: False                    # if True, save the trained model to the path specified in the architecture script. Else, do not save the model.
    graph : 'laman_knn_graph'            # Choices: 'laman_knn_graph' (best) , 'laman_random_graph' ,  '2n3_nearest_neighbors', 'knn_graph'
    k : 2                                # If graph = 'knn_graph': Number of nearest neighbors to consider in the graph construction. 
    sorting_key: 'pt' # If graph = laman_knn_graph, this is the key to sort the particles before constructing the graph. 
                                         # Choices: 'pt', 'angularity_increasing' , 'angularity_decreasing' 

    add_angles: 0       # How many angles to add (+) or remove (-) from the Laman Graph. Currently only works with "random_graph: False" 
                        # The default is 0: When add_angles < 0 and  abs(add_angles)> n_particles^jet - 2, the graph will be disconnected, so in the code
                        # we set remove_angles = min(remove_angles, n_particles^jet - 2) to avoid this issue. 
                        # The average jet has ~40 particles and only ~2% of the jets have less than 15 particles.
                        # Conversely when add_angles > (n_particles^jet choose 2) - (2*n_particles^jet -3), the graph will be fully connected, so we stop there.
                      
particle_net:
    graph_types: ['disconnected'] # placeholder name for the graph type since no actual graph is used as input. The graph is constructed in the architecture script
    batch_size: 512
    epochs: 10
    learning_rate: 0.001
    three_momentum_features: True # if True, use the three momentum features (pt, eta, phi) as input features for the PN. Else, use the original 7dim feature space.
    original_train_parameters: False # if True, use the original training parameters from the paper https://arxiv.org/abs/1902.08570. So far the best hyperparameters for the particle_net model

particle_net_laman:
    graph_types: ['disconnected'] # placeholder name for the graph type since no actual graph is used
    batch_size: 512
    epochs: 10
    learning_rate: 0.001
    three_momentum_features: True # This has to be True. If set to False, it will be overwritten to True.
    original_train_parameters: False # if True, use the original training parameters from the paper https://arxiv.org/abs/1902.08570. So far the best hyperparameters for the particle_net model


particle_gcn_pytorch:
    graph_types: ['fully_connected']
    edge_features: False
    hidden_dim: 64
    batch_size: 512
    epochs: 15
    learning_rate: 0.001

particle_gat_pytorch:
    graph_types: ['fully_connected']
    edge_features: False
    hidden_dim: 8
    n_heads: 8
    batch_size: 512
    epochs: 15
    learning_rate: 0.001

subjet_gcn_pytorch:
    graph_types: ['laman_naive', 'laman_1N', 'laman_1N2N']
    edge_features: False
    hidden_dim: 64
    batch_size: 128
    epochs: 5
    learning_rate: 0.001

subjet_gat_pytorch:
    graph_types: ['laman_naive', 'laman_1N', 'laman_1N2N']
    edge_features: True
    hidden_dim: 8
    n_heads: 8
    batch_size: 128
    epochs: 5    
    learning_rate: 0.001
